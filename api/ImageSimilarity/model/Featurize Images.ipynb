{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize Images for Image Similarity Model\n",
    "\n",
    "This notebook will do the following:\n",
    "\n",
    "For each image in the provided Azure Blob container:\n",
    "    - download the image\n",
    "    - resize the image to the pre-defined img_width & img_height\n",
    "    - Featurize the image using the Keras pre-trained ResNet50 model trained on imagenet\n",
    "    - save the featurized images to a preprocessedimages.pkl file in the provide data directory\n",
    "    - save a corresponding targets.pkl file with a table of the [name, url] for each image \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image, ImageFile\n",
    "from urllib.request import urlopen\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from azure.storage.blob import BlockBlobService, PublicAccess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_account_name = 'mmlsparkdemo' #Azure storage account name\n",
    "blob_sas_token='?st=2019-10-09T18%3A54%3A49Z&se=2020-10-10T18%3A54%3A00Z&sp=rl&sv=2018-03-28&sr=c&sig=sC4kVoSxN93Wd2x4PUCfodMHs2VG6p5%2BEDdIkNUrpTA%3D' # SAS token to access the blob\n",
    "blob_container = 'met' #container where the images are located\n",
    "blob_prefix = 'thumbnails/' #path to any sub-folders within the container \n",
    "\n",
    "batch_size = 64 \n",
    "img_width = 512 #input sizes images will be re-szied to\n",
    "img_height = 512\n",
    "\n",
    "output_root = '/mnt/met-results/' #root folder filepath. All data from this notebookwill be saved to this directory\n",
    "features_fn = output_root + 'features.pkl' #name of the np array of size (sample_length, img_length) will be saved. These are the featurized versions of the images\n",
    "files_fn = output_root + 'filenames.pkl' # helper table that tracks the name & URL for each row\n",
    "\n",
    "\n",
    "#check that the all the variables have been set\n",
    "assert blob_account_name != '', 'Please provide the Azure storage account name where the images are stored'\n",
    "assert blob_sas_token != '', 'Please provide the SAS token for accessing the blob account'\n",
    "assert blob_container != '', 'Please provide the container name where the images are stored'\n",
    "assert blob_prefix !='', 'Please provide any additional path compnoents for the imates. Example if the iamges are stored in containername/data/images the prefix is data/images'\n",
    "assert output_root != '', 'Please provide a filepath for where the data should be saved. Example: /data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize Resnet50 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize model\n",
    "img_length = 2048 #size of output from model\n",
    "keras_model = ResNet50(input_shape=[img_width,img_height,3], \n",
    "                     weights='imagenet', \n",
    "                     include_top=False, \n",
    "                     pooling='avg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Featurize images with ResNet50 Model & Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 170664 files\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "#Connect to the blob\n",
    "block_blob_service = BlockBlobService(account_name=blob_account_name, sas_token=blob_sas_token)\n",
    "files = list(block_blob_service.list_blobs(blob_container, prefix=blob_prefix))\n",
    "n_files = len(files)\n",
    "print(\"Found {} files\".format(n_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  34/2667 [..............................] - ETA: 1:30:52"
     ]
    }
   ],
   "source": [
    "import math \n",
    "\n",
    "urls = (\"https://{}.blob.core.windows.net/{}/{}\".format(blob_account_name, blob_container, file.name) for file in files)\n",
    "\n",
    "def preprocess_image(url):\n",
    "    try:\n",
    "        with urlopen(url) as file:\n",
    "            img = Image.open(file)\n",
    "\n",
    "        #non RGB images won't have the right number of channels\n",
    "        if img.mode != 'RGB': \n",
    "            img = img.convert('RGB') \n",
    "\n",
    "        #re-size, expand dims and run through the ResNet50 model\n",
    "        img = np.array(img.resize((img_width, img_height)))\n",
    "        img = preprocess_input(img.astype(np.float))\n",
    "        obj_id = url.split(\"/\")[-1].split(\".\")[0]\n",
    "        return (img, obj_id)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None\n",
    "\n",
    "preprocessed = (preprocess_image(url) for url in urls)\n",
    "\n",
    "obj_ids = []\n",
    "def batch(iterable, n):\n",
    "    current_batch = []\n",
    "    for item in iterable:\n",
    "        if item is not None:\n",
    "            current_batch.append(item[0])\n",
    "            obj_ids.append(item[1])\n",
    "            if len(current_batch) == n:\n",
    "                yield np.array(current_batch)\n",
    "                current_batch = []\n",
    "    if current_batch:\n",
    "        yield np.array(current_batch)\n",
    "\n",
    "\n",
    "batches = batch(preprocessed, batch_size)\n",
    "\n",
    "predictions = keras_model.predict_generator(batches, steps = math.ceil(n_files/batch_size), verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists(output_root): os.makedirs(output_root)\n",
    "pickle.dump(predictions, open(features_fn, 'wb'))\n",
    "pickle.dump(obj_ids,open(files_fn,'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
